knitr::opts_chunk$set(echo = FALSE)
library(survival)
library(flexsurv)
library(survMisc) # for Wilcoxon-Gehan-Breslow test
source("/Users/david/Documents/GitHub/Survival/Homework 3/getmedianres.R")
luke <- read.csv("/Users/david/Documents/GitHub/Survival/Homework 3/ccg803.csv", header=TRUE)
surv.luke <- Surv(time=luke$duration, event=luke$relapse, type="right")
model.1 <- coxph(surv.luke ~ rx, data=luke)
summary(model.1)
model.2 <- coxph(surv.luke ~ rx + wbc + age, data=luke)
summary(model.2)
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + rx_high + rx_low + wbc_high + wbc_low + age, data = luke)
summary(model.3)
exp(coef(model.3)[rx_high])
#larger model
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + rx_high + rx_low + wbc_high + wbc_low + age, data = luke)
summary(model.3)
exp(coef(model.3)[rx_high])
#larger model
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + rx_high + rx_low + wbc_high + wbc_low + age, data = luke)
summary(model.3)
exp(coef(model.3)[model.3$rx_high])
model.3_est <- exp(coef(model.3)[model.3$rx_high])
model.3_est
#larger model
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + rx_high + rx_low + wbc_high + wbc_low + age, data = luke)
summary(model.3)
model.3_est <- exp(coef(model.3)[rx_high])
#larger model
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + rx_high + rx_low + wbc_high + wbc_low + age, data = luke)
summary(model.3)
model.3_est <- exp(coef(model.3)[luke$rx_high])
model.3_est
model.3_est
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + rx_high + rx_low + wbc_high + wbc_low + age, data = luke)
summary(model.3)
model.3_est <- exp(coef(model.3)["rx_high"])
model.3_est
summary(model.3)
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ rx + wbc_high + wbc_low + rx_high + rx_low + age, data = luke)
summary(model.3)
summary(model.3)
View(surv.luke)
View(luke)
#larger model
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ wbc_high + wbc_low + rx_high + rx_low + age, data = luke)
summary(model.3)
model.3_est <- exp(coef(model.3)["rx_high"])
model.3_est
model.3 <- coxph(surv.luke ~ wbc_high + wbc_low + rx_high + rx_low + age, data = luke)
summary(model.3)
model.3_est <- exp(coef(model.3)["rx_high"])
model.3_est
#larger model
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
model.3 <- coxph(surv.luke ~ wbc_high + wbc_low + rx_high + rx_low + age, data = luke)
summary(model.3)
model.3_est <- exp(coef(model.3)["rx_high"])
model.3_est
model.3_small <- coxph(surv.luke ~ rx + wbc_high + wbc_low + age, data=luke)
summary(model.3_small)
as.numeric(2 * (logLik(model.3_large) - logLik(model.3_small)))
#creating interaction variables
luke$wbc_high <- (luke$wbc >= 100)*luke$wbc
luke$wbc_low <- (luke$wbc < 100)*luke$wbc
luke$rx_high <- (luke$wbc >= 100)*luke$rx
luke$rx_low <- (luke$wbc < 100)*luke$rx
#model with interaction
model.3_large <- coxph(surv.luke ~ wbc_high + wbc_low + rx_high + rx_low + age, data = luke)
summary(model.3)
model.3_large_est <- exp(coef(model.3)["rx_high"])
model.3_large_est
#model without interaction
model.3_small <- coxph(surv.luke ~ rx + wbc_high + wbc_low + age, data=luke)
summary(model.3_small)
#performing test to determine interaction
as.numeric(2 * (logLik(model.3_large) - logLik(model.3_small)))
as.numeric(2 * (logLik(model.3_large) - logLik(model.3_small)))
model.4 <- coxph(surv.luke ~ rx + wbc + age + institution, data=luke)
summary(model.4)
model.4 <- coxph(surv.luke ~ rx + wbc + age + strata(institution), data=luke)
summary(model.4)
plot(survfit(model.2,
newdata=data.frame(rx=1, age=5, wbc=45)),
conf.int=FALSE, lwd=2, col="blue", lty="dashed",
xlab="Days", ylab="Survival Probability")
lines(survfit(model.2,
newdata=data.frame(rx=0, age=5, wbc=45)),
conf.int=FALSE, lwd=2, col="blue", lty="solid")
lines(survfit(model.2,
newdata=data.frame(rx=1, age=5, wbc=210)),
conf.int=FALSE, lwd=2, col="orange", lty="dashed")
lines(survfit(model.2,
newdata=data.frame(rx=0, age=5, wbc=210)),
conf.int=FALSE, lwd=2, col="orange", lty="solid")
knitr::opts_chunk$set(echo = FALSE)
if (!isTRUE(requireNamespace("INLA", quietly = TRUE))) {
install.packages("INLA", repos = c(getOption("repos"),
INLA = "https://inla.r-inla-download.org/R/stable"),
dep=TRUE)
}
if (!require(SUMMER)) install.packages("SUMMER", repos = "http://cran.us.r-project.org")
if (!require(foreign)) install.packages("foreign", repos = "http://cran.us.r-project.org")
if (!require(haven)) install.packages("haven")
if (!require(rgdal)) install.packages("rgdal")
if (!require(maptools)) install.packages("maptools")
if (!require(sp)) install.packages("sp")
if (!require(spdep)) install.packages("spdep")
if (!require(SpatialEpi)) install.packages("SpatialEpi")
if (!require(RColorBrewer)) install.packages("RColorBrewer")
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(maps)) install.packages("maps")
if (!require(broom)) install.packages("broom")
if (!require(raster)) install.packages("raster")
if (!require(leaflet)) install.packages("leaflet")
if (!require(dplyr)) install.packages("dplyr")
if (!require(tidyverse)) install.packages("tidyverse")
rm(list=ls())
library(foreign)
library(haven)
library(INLA)
library(rgdal)
library(maptools)
library(sp)
library(spdep)
library(SpatialEpi)
library(RColorBrewer)
library(ggplot2)
library(maps)
library(broom)
library(raster)
library(leaflet)
library(dplyr)
library(tidyverse)
library(SUMMER)
#Cancer data
#link = "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Data/ohio_data_ascending_fips.txt"
#ohio_canc <- read.table(url(link), header=TRUE)
#Cancer data
#on pc at home
#ohio_canc <- read.table("/Users/david/Documents/GitHub/Spatial_epi/HW 3/Data/ohio_2019_version.txt", header=TRUE)
#on laptop
#ohio_canc <- read.table("/Users/david/Documents/GitHub/Spatial_epi/HW 3/Data/ohio_2019_version.txt", header=TRUE)
#using github
link = "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Data/ohio_2019_version.txt"
ohio_canc <- read.table(url(link), header=TRUE)
#Map data
zip_oh_map <- "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Map%20data.zip"
#library(utils)
#temp=tempfile()
#download.file(zip_oh_map, temp)
#unzip(temp)
#(maps=list.files(pattern = 'shp'))
#ohmap <- readOGR(url(zip_oh_map),stringsAsFactors=F)
#maplink <- "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Map%20data/"
#ohmap <- readOGR(dsn = maplink, layer="ohio_map")
#pc in office
ohmap <- readOGR(dsn="C:\\Users\\dcoomes\\Dropbox\\Classes\\Spatial modeling\\HW 3\\Data\\Map data", layer="ohio_map")
#Cancer data
#link = "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Data/ohio_data_ascending_fips.txt"
#ohio_canc <- read.table(url(link), header=TRUE)
#Cancer data
#on pc at home
#ohio_canc <- read.table("/Users/david/Documents/GitHub/Spatial_epi/HW 3/Data/ohio_2019_version.txt", header=TRUE)
#on laptop
#ohio_canc <- read.table("/Users/david/Documents/GitHub/Spatial_epi/HW 3/Data/ohio_2019_version.txt", header=TRUE)
#using github
link = "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Data/ohio_2019_version.txt"
ohio_canc <- read.table(url(link), header=TRUE)
#Map data
zip_oh_map <- "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Map%20data.zip"
#library(utils)
#temp=tempfile()
#download.file(zip_oh_map, temp)
#unzip(temp)
#(maps=list.files(pattern = 'shp'))
#ohmap <- readOGR(url(zip_oh_map),stringsAsFactors=F)
#maplink <- "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Map%20data/"
#ohmap <- readOGR(dsn = maplink, layer="ohio_map")
#pc in office
#ohmap <- readOGR(dsn="C:\\Users\\dcoomes\\Dropbox\\Classes\\Spatial modeling\\HW 3\\Data\\Map data", layer="ohio_map")
#laptop
ohmap <- readOGR(dsn="/Users/david/Documents/GitHub/Spatial_epi/HW 3/Map data", layer="ohio_map")
#ordering of regions is not the same among the data sets - how do we align these two?
#summary(ohmap)
#ohmap$COUNTYFP00
#summary(ohio_canc)
#ohio_canc$fips
#View(ohio_canc)
#creating smr
ohio_canc$smr <- ohio_canc$Obs/ohio_canc$Exp
#ohmap <- readOGR(dsn="C:\\Users\\dcoomes\\Documents\\GitHub\\Spatial_epi\\HW 3\\Data\\Map data", #layer="ohio_map")
#When using PC in office
#ohmap <- readOGR(dsn="C:\\Users\\dcoomes\\Desktop\\Epi PhD\\Classes\\Spatial epi\\Map data", layer="ohio_map")
#ordering of regions is not the same among the data sets - how do we align these two?
summary(ohmap)
ohmap$CNTYIDFP00
#summary(ohio_canc)
#ohio_canc$fips
#View(ohio_canc)
#creating RR
ohio_canc$Z <- log(ohio_canc$smr)
ohio_canc$varZ <- 1/(ohio_canc*ohio_canc$smr)
ohio_canc$precZ <- 1/ohio_canc$varZ
ohmap$fips <- as.numeric(as.character(ohmap$CNTYIDFP00))
ohmap <- ohmap[order(ohmap$fips),]            #ordering fips so that counties align in data sets
ohmap <- merge(ohmap,ohio_canc,by="fips")
#creating graph file (what is this doing?)
nb.map <- poly2nb(ohmap)
nb2INLA("ohmap.graph", nb.map)
#creating region variable in the cancer data
#ohio_canc <- ohio_canc[order(ohio_canc$fips),]     #not sure if I need to do this?
ohio_canc$Region <- 1:nrow(ohmap)
#I get twice the number of regions as there are (176 as opposed to 88)
#ohmap$Region <- ohmap$fips
#for this we are fitting a spatially smoothed ICAR model using the INLA function
#Don't need to define pc.prec for this question
#pc.prec <- list(theta = list(prior = "pc.prec",
#               param = c(1, 0.05)))
#ohmap2 <- as.data.frame(ohmap)
#setting up the inla model
formula <- Obs ~ 1 +
f(Region, model="bym2", graph="ohmap.graph", scale.model=T, constr=T,
hyper=list(phi=list(prior="pc",
param=c(0.5, 0.5), initial=1), prec=list(prior="pc.prec",
param=c(0.3,0.01), initial=5)))
#fit the inla model
ohio.fit1 <- inla(formula, data=ohio_canc, family="poisson",
E=Exp,
control.predictor=list(compute=TRUE))
#ohio.fit1 <- inla(formula, data=ohio_canc,
#              family="gaussian",
#             control.predictor=list(compute=TRUE),
#            control.family = list(hyper = list(prec = list(initial = log(1), fixed=TRUE))),scale=precZ)
#get estimates
summary(ohio.fit1)
post.med.1 <- ohio.fit1$summary.fixed[4]                       #posterior med
sdmed.1 <- 1/sqrt(ohio.fit1$summary.hyperpar$`0.5quant`[1])     #posterior sd
beta_low <- post.med.1 - 1.96*sdmed.1
beta_high <- post.med.1 + 1.96*sdmed.1
re.var <- 1/ohio.fit1$summary.hyperpar$`0.5quant`[1]      #variance of the random effects
prop.spa <- ohio.fit1$summary.hyperpar$`0.5quant`[2]      #proportion of random effects due to spatial
sdmed.1 <- 1/sqrt(ohio.fit1$summary.hyperpar$`0.5quant`[1])     #posterior sd
sdmed.1
ohio.fit1$summary.hyperpar
ohio.fit1$summary.hyperpar
diff <- ohio.fit1$summary.random$Region[1:88, 2] -
ohio.fit1$summary.random$Region[89:176, 2]
REsnonspat <- exp(diff)
REsspat <- exp(ohio.fit1$summary.random$Region[89:176, 5])
ohmap$REsnonspat <- REsnonspat
ohmap$REsspat <- REsspat
spplot(ohmap, c("REsnonspat"), col.regions=colorRampPalette(rev(brewer.pal(8, "RdBu")))(50))
#reading in BRFSS data
data(BRFSS)
#summary(BRFSS)
#removing obs with missing hracode and smoking status
brfss <- subset(BRFSS, !is.na(BRFSS$hracode))
brfss <- subset(brfss, !is.na(brfss$smoker1))
#load spatial data
data(KingCounty)
nb.r <- poly2nb(KingCounty, queen=F,
row.names = KingCounty$HRA2010v2_)
mat <- nb2mat(nb.r, style="B", zero.policy = TRUE)
colnames(mat) <- rownames(mat)
mat <- as.matrix(mat[1:dim(mat)[1], 1:dim(mat)[1]])
mat[1:2, 1:2]
#calculating weighted non-smoothed estimates
library(survey)
design <- svydesign(ids=~1, weights = ~rwt_llcp,
strata = ~strata, data=brfss)
direct <- svyby(~smoker1, ~hracode, design, svymean)
head(direct, n=2)
#smoothed, non-weighted estimates and non-smoothed, non-weighted estimates
smoothed <- fitGeneric(data=brfss, geo=KingCounty,
Amat=mat, responseType = "binary", responseVar="smoker1",
strataVar=NULL, weightVar=NULL, regionVar="hracode",
clusterVar=NULL, CI=0.95)
head(smoothed$HT, n=2)
#The smooth estimates are the smoothed non-weighted estimates and the HT estimates are the non-smoothed, non-weighted estimates
head(smoothed$smooth, n=1)
#simple smoothed binomial probabilities
head(smoothed$HT, n=1)
install.packages('rgeos', type='source')
install.packages('rgdal', type='source')
library(rgeos)
install.packages("rgdal", type = "source")
toplot <- smoothed$smooth
mapPlot(data = toplot, geo=KingCounty,
variables=c("mean.original"),
labels=c("Posterior Mean"), by.data="region",
by.geo="HRA2010v2_")
if (!isTRUE(requireNamespace("INLA", quietly = TRUE))) {
install.packages("INLA", repos = c(getOption("repos"),
INLA = "https://inla.r-inla-download.org/R/stable"),
dep=TRUE)
}
if (!require(SUMMER)) install.packages("SUMMER", repos = "http://cran.us.r-project.org")
if (!require(foreign)) install.packages("foreign", repos = "http://cran.us.r-project.org")
if (!require(haven)) install.packages("haven")
if (!require(rgeos)) install.packages("rgeos")
if (!require(rgdal)) install.packages("rgdal")
if (!require(maptools)) install.packages("maptools")
if (!require(sp)) install.packages("sp")
if (!require(spdep)) install.packages("spdep")
if (!require(SpatialEpi)) install.packages("SpatialEpi")
if (!require(RColorBrewer)) install.packages("RColorBrewer")
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(maps)) install.packages("maps")
if (!require(broom)) install.packages("broom")
if (!require(raster)) install.packages("raster")
if (!require(leaflet)) install.packages("leaflet")
if (!require(dplyr)) install.packages("dplyr")
if (!require(tidyverse)) install.packages("tidyverse")
rm(list=ls())
library(foreign)
library(haven)
library(INLA)
library(rgdal)
library(maptools)
library(sp)
library(spdep)
library(SpatialEpi)
library(RColorBrewer)
library(ggplot2)
library(maps)
library(broom)
library(raster)
library(leaflet)
library(dplyr)
library(tidyverse)
library(SUMMER)
library(rgeos)
#reading in BRFSS data
data(BRFSS)
#summary(BRFSS)
#removing obs with missing hracode and smoking status
brfss <- subset(BRFSS, !is.na(BRFSS$hracode))
brfss <- subset(brfss, !is.na(brfss$smoker1))
#load spatial data
data(KingCounty)
nb.r <- poly2nb(KingCounty, queen=F,
row.names = KingCounty$HRA2010v2_)
mat <- nb2mat(nb.r, style="B", zero.policy = TRUE)
colnames(mat) <- rownames(mat)
mat <- as.matrix(mat[1:dim(mat)[1], 1:dim(mat)[1]])
mat[1:2, 1:2]
#calculating weighted non-smoothed estimates
library(survey)
design <- svydesign(ids=~1, weights = ~rwt_llcp,
strata = ~strata, data=brfss)
direct <- svyby(~smoker1, ~hracode, design, svymean)
head(direct, n=2)
#smoothed, non-weighted estimates and non-smoothed, non-weighted estimates
smoothed <- fitGeneric(data=brfss, geo=KingCounty,
Amat=mat, responseType = "binary", responseVar="smoker1",
strataVar=NULL, weightVar=NULL, regionVar="hracode",
clusterVar=NULL, CI=0.95)
head(smoothed$HT, n=2)
#The smooth estimates are the smoothed non-weighted estimates and the HT estimates are the non-smoothed, non-weighted estimates
head(smoothed$smooth, n=1)
#simple smoothed binomial probabilities
head(smoothed$HT, n=1)
#install.packages('rgeos', type='source')
#install.packages('rgdal', type='source')
#library(rgeos)
#mapping smoothed, non-weighted estimates
toplot <- smoothed$smooth
mapPlot(data = toplot, geo=KingCounty,
variables=c("mean.original"),
labels=c("Posterior Mean"), by.data="region",
by.geo="HRA2010v2_")
if (!require(gpclib)) install.packages("gpclib", type="source")
gpclibPermit()
toplot <- smoothed$smooth
mapPlot(data = toplot, geo=KingCounty,
variables=c("mean.original"),
labels=c("Posterior Mean"), by.data="region",
by.geo="HRA2010v2_")
svysmoothed <- fitGeneric(data=brfss, geo=KingCounty,
Amat = mat, responseType="binary", responseVar="diab2",
strataVar="strata", weightVar="rwt_11cp", regionVar="hracode",
clusterVar="~1", CI=0.95)
if (!require(gpclib)) install.packages("gpclib", type="source")
gpclibPermit()
#mapping smoothed, non-weighted estimates
toplot <- smoothed$smooth
mapPlot(data = toplot, geo=KingCounty,
variables=c("mean.original"),
labels=c("Posterior Mean"), by.data="region",
by.geo="HRA2010v2_")
svysmoothed <- fitGeneric(data=brfss, geo=KingCounty,
Amat = mat, responseType="binary", responseVar="diab2",
strataVar="strata", weightVar="rwt_11cp", regionVar="hracode",
clusterVar="~1", CI=0.95)
svysmoothed <- fitGeneric(data=brfss, geo=KingCounty,
Amat = mat, responseType="binary", responseVar="diab2",
strataVar="strata", weightVar="rwt_llcp", regionVar="hracode",
clusterVar="~1", CI=0.95)
svysmoothed <- fitGeneric(data=brfss, geo=KingCounty,
Amat = mat, responseType="binary", responseVar="diab2",
strataVar="strata", weightVar="rwt_llcp", regionVar="hracode",
clusterVar="~1", CI=0.95)
est <- data.frame(naive=smoothed$HT$HT.est.original,
weighted=svysmoothed$HT$HT.est.original, smooth=smoothed$smooth$mean.original,
weightedsmooth=svysmoothed$smooth$mean.original)
var <- data.frame(naive=smoothed$HT$HT.variance.original,
weighted=svysmoothed$HT$HT.variance.original,
smooth=smoothed$smooth$variance.original, weightedsmooth=svysmoothed$smooth)
summary(ohio.fit1)
formula <- Obs ~ 1 +
f(Region, model="bym2", graph="ohmap.graph", scale.model=T, constr=T,
hyper=list(phi=list(prior="pc",
param=c(0.5, 0.5), initial=1), prec=list(prior="pc.prec",
param=c(0.3,0.01), initial=5)))
#fit the inla model
ohio.fit1 <- inla(formula, data=ohio_canc, family="poisson",
E=Exp,
control.predictor=list(compute=TRUE))
#using github
link = "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Data/ohio_2019_version.txt"
ohio_canc <- read.table(url(link), header=TRUE)
#Map data
zip_oh_map <- "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Map%20data.zip"
#library(utils)
#temp=tempfile()
#download.file(zip_oh_map, temp)
#unzip(temp)
#(maps=list.files(pattern = 'shp'))
#ohmap <- readOGR(url(zip_oh_map),stringsAsFactors=F)
#maplink <- "https://github.com/dmccoomes/Spatial_epi/raw/master/HW%203/Map%20data/"
#ohmap <- readOGR(dsn = maplink, layer="ohio_map")
#pc in office
#ohmap <- readOGR(dsn="C:\\Users\\dcoomes\\Dropbox\\Classes\\Spatial modeling\\HW 3\\Data\\Map data", layer="ohio_map")
#laptop
ohmap <- readOGR(dsn="/Users/david/Documents/GitHub/Spatial_epi/HW 3/Map data", layer="ohio_map")
#creating smr
ohio_canc$smr <- ohio_canc$Obs/ohio_canc$Exp
#ordering of regions is not the same among the data sets - how do we align these two?
summary(ohmap)
ohmap$CNTYIDFP00
#summary(ohio_canc)
#ohio_canc$fips
#View(ohio_canc)
#creating RR
ohio_canc$Z <- log(ohio_canc$smr)
ohio_canc$varZ <- 1/(ohio_canc*ohio_canc$smr)
ohio_canc$precZ <- 1/ohio_canc$varZ
ohmap$fips <- as.numeric(as.character(ohmap$CNTYIDFP00))
ohmap <- ohmap[order(ohmap$fips),]            #ordering fips so that counties align in data sets
ohmap <- merge(ohmap,ohio_canc,by="fips")
#creating graph file (what is this doing?)
nb.map <- poly2nb(ohmap)
nb2INLA("ohmap.graph", nb.map)
#creating region variable in the cancer data
#ohio_canc <- ohio_canc[order(ohio_canc$fips),]     #not sure if I need to do this?
ohio_canc$Region <- 1:nrow(ohmap)
#I get twice the number of regions as there are (176 as opposed to 88)
#setting up the inla model
formula <- Obs ~ 1 +
f(Region, model="bym2", graph="ohmap.graph", scale.model=T, constr=T,
hyper=list(phi=list(prior="pc",
param=c(0.5, 0.5), initial=1), prec=list(prior="pc.prec",
param=c(0.3,0.01), initial=5)))
#fit the inla model
ohio.fit1 <- inla(formula, data=ohio_canc, family="poisson",
E=Exp,
control.predictor=list(compute=TRUE))
#get estimates
summary(ohio.fit1)
ohio.fit1$summary.hyperpar
ohio.fit1$summary.fixed
ohio.fit1$summary.hyperpar
var.random.1 <- 1/sqrt(ohio.fit1$summary.hyperpar$`0.5quant`[1])     #total variance of random effects
prop.1 <- ohio.fit1$summary.hyperpar$`0.5quant`[2])
prop.1 <- ohio.fit1$summary.hyperpar$`0.5quant`[2]
prop.1
